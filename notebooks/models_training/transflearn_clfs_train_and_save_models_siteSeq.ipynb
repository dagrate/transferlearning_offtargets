{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw-wm8I1n9Mo"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# ===============================================================\n",
        "# ===       NOTEBOOK FOR SAVING THE HYPERTUNED MODELS         ===\n",
        "# ===                      FOR SITE SEQ                       ===\n",
        "# ===============================================================\n",
        "# ===============================================================\n",
        "\n",
        "__date__= '30-Oct-22'\n",
        "__author__='jeremy charlier'\n",
        "__revised__='17-Nov-22'\n",
        "\n",
        "\"\"\"comments\n",
        "the encoder class is inherited from\n",
        "[1]: \"CRISPR-Net: A Recurrent Convolutional Network Quantiï¬es\n",
        "CRISPR Off-Target Activities with Mismatches and Indels\", J. Lin et al\n",
        "https://onlinelibrary.wiley.com/doi/epdf/10.1002/advs.201903562\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "path_to_module = 'MODULE_PATH'   # append drive directory to python sys path\n",
        "sys.path.append(path_to_module)\n",
        "sys.path.append(path_to_module+'/code/')   # location of python source code\n",
        "sys.path.append(path_to_module)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "from joblib import dump\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier as MLP\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.utils import Bunch\n",
        "from sklearn.metrics import (\n",
        "\tclassification_report, roc_auc_score,\n",
        "\tconfusion_matrix, f1_score,\n",
        "\troc_curve, precision_score, recall_score,\n",
        "\tauc, average_precision_score,\n",
        "\tprecision_recall_curve, accuracy_score)\n",
        "from imblearn.under_sampling import RandomUnderSampler as rus\n",
        "#\n",
        "from transferlearning_datapipeline import datapipeline\n",
        "from simanalysis import getBootStrappedData\n",
        "from transferlearning_modelpipeline import modelpipeline\n",
        "import transferlearning_utils as tlrn\n",
        "import transferlearning_tensorflow_models as tf_models\n",
        "#\n",
        "p = print"
      ],
      "metadata": {
        "id": "BhK5vLFpoPLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rebalanceData(\n",
        "    xtrain, ytrain,\n",
        "    is_sampling = True,\n",
        "    sampling_ratio = 0.8):\n",
        "  # change sampling strat for CIRCLE-SEQ (itrain==1)\n",
        "  # sampling_ratio = 0.2 else 0.8\n",
        "  if is_sampling:\n",
        "    xtrainres, ytrainres = rus(\n",
        "      sampling_strategy = sampling_ratio,\n",
        "      random_state = 0\n",
        "    ).fit_resample(xtrain, ytrain)\n",
        "  else:\n",
        "    xtrainres, ytrainres = xtrain, ytrain\n",
        "  return xtrainres, ytrainres\n",
        "# end function\n",
        "#\n",
        "def getFprTpr(estimator, disp = False):\n",
        "  preds = estimator.yscore\n",
        "  if len(preds.shape) == 2:\n",
        "    ypreds = preds[:,1]\n",
        "  else:\n",
        "    ypreds = preds\n",
        "  fprs, tprs, _ = roc_curve(estimator.y_test, ypreds)\n",
        "  if disp:\n",
        "    print('false positive rates:\\n', fprs)\n",
        "    print('true positive rates:\\n', tprs)\n",
        "  return fprs, tprs\n",
        "#"
      ],
      "metadata": {
        "id": "5l8opHsdotbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def urnd(x): return np.round(x, 3);\n",
        "#\n",
        "def plotRocCurve(\n",
        "\t\testimators,\n",
        "    icol = 1, disp = False):\n",
        "  nwpos = 0\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  # +++ MLP 1 layer +++\n",
        "  fprs, tprs = getFprTpr( estimators[nwpos], disp = disp )\n",
        "  plt.plot(\n",
        "    fprs, tprs,\n",
        "    label = 'MLP 1 layer (AUC: %s \\u00B1 0.001)' % (urnd(auc(fprs, tprs))))\n",
        "  # +++ MLP 2 layers +++\n",
        "  nwpos += 1\n",
        "  fprs, tprs = getFprTpr( estimators[nwpos], disp = disp )\n",
        "  plt.plot(\n",
        "    fprs, tprs,\n",
        "    label = 'MLP 2 layers (AUC: %s \\u00B1 0.001)' % (urnd(auc(fprs, tprs))))\n",
        "  # +++ RF +++\n",
        "  nwpos += 1\n",
        "  fprs, tprs = getFprTpr( estimators[nwpos], disp = disp )\n",
        "  plt.plot(\n",
        "    fprs, tprs,\n",
        "    label = 'RF (AUC: %s \\u00B1 0.001)' % (urnd(auc(fprs, tprs))))\n",
        "  # +++ LR model in position 3 +++\n",
        "  nwpos += 1\n",
        "  fprs, tprs = getFprTpr( estimators[nwpos], disp = disp )\n",
        "  plt.plot(\n",
        "    fprs, tprs,\n",
        "    label='LR (AUC: %s \\u00B1 0.001)' % (urnd(auc(fprs, tprs))))\n",
        "  # dllab = [\n",
        "  #   'FFN3', 'FFN5', 'FFN10',\n",
        "  #   'CNN3', 'CNN5', 'CNN10',\n",
        "  #   'LSTM', 'GRU']\n",
        "  # for nwpos in range(4, 12):\n",
        "  #   clf = estimators[nwpos]\n",
        "  #   plt.plot(\n",
        "  #     clf.fpr, clf.tpr,\n",
        "  #     label = dllab[nwpos-4] + ' (AUC: %s \\u00B1 0.001)' % (clf.roc_auc))\n",
        "  # +++ FNN3 model in position 4 +++\n",
        "  nwpos += 1\n",
        "  clf = estimators[nwpos]\n",
        "  plt.plot(\n",
        "    clf.fpr, clf.tpr,\n",
        "    label='FFN3 (AUC: %s \\u00B1 0.001)' % (clf.roc_auc))\n",
        "  # +++ FNN5 model in position 5 +++\n",
        "  nwpos += 1\n",
        "  clf = estimators[nwpos]\n",
        "  plt.plot(\n",
        "    clf.fpr, clf.tpr,\n",
        "    label='FFN5 (AUC: %s \\u00B1 0.001)' % (clf.roc_auc))\n",
        "  # FNN10 model in position 6\n",
        "  nwpos += 1\n",
        "  clf = estimators[nwpos]\n",
        "  plt.plot(\n",
        "    clf.fpr, clf.tpr,\n",
        "    label='FFN10 (AUC: %s \\u00B1 0.001)' % (clf.roc_auc))\n",
        "  # +++ CNN3 model in position 7 +++\n",
        "  nwpos += 1\n",
        "  clf = estimators[nwpos]\n",
        "  plt.plot(\n",
        "    clf.fpr, clf.tpr,\n",
        "    label = 'CNN3 (AUC: %s \\u00B1 0.001)' % (clf.roc_auc))\n",
        "  # +++ CNN5 model in position 8 +++\n",
        "  nwpos += 1\n",
        "  clf = estimators[nwpos]\n",
        "  plt.plot(\n",
        "    clf.fpr, clf.tpr,\n",
        "    label = 'CNN5 (AUC: %s \\u00B1 0.001)' % (clf.roc_auc))\n",
        "  # +++ CNN10 model in position 9 +++\n",
        "  nwpos += 1\n",
        "  clf = estimators[nwpos]\n",
        "  plt.plot(\n",
        "    clf.fpr, clf.tpr,\n",
        "    label = 'CNN10 (AUC: %s \\u00B1 0.001)' % (clf.roc_auc))\n",
        "  # +++ LSTM model in position 10 +++\n",
        "  nwpos += 1\n",
        "  clf = estimators[nwpos]\n",
        "  plt.plot(\n",
        "    clf.fpr, clf.tpr,\n",
        "    label = 'LSTM (AUC: %s \\u00B1 0.001)' % (clf.roc_auc))\n",
        "  # +++ GRU model in position 11 +++\n",
        "  nwpos += 1\n",
        "  clf = estimators[nwpos]\n",
        "  plt.plot(\n",
        "    clf.fpr, clf.tpr,\n",
        "    label = 'GRU (AUC: %s \\u00B1 0.001)' % (clf.roc_auc))\n",
        "  # +++ plot legend +++\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.legend(loc='best')\n",
        "  # plt.show()\n",
        "  plt.savefig('plot1.pdf')\n",
        "#"
      ],
      "metadata": {
        "id": "NVVS10cq0uU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === VARIABLE INITIALIZATION ===\n",
        "issavemodels = True # save models (Y=True / N=False)\n",
        "issklearnmodels = False # for sklearn models\n",
        "isfnnmodels = False # for FNNs models\n",
        "iscnnmodels = True # for CNNs models\n",
        "isrnnmodels = True # for RNNs models\n",
        "#\n",
        "# === READ ENCODED DATA ===\n",
        "is_read_pkl_encoded_data = True\n",
        "if is_read_pkl_encoded_data:\n",
        "  f = open(path_to_module+'/data/encoded_data.pkl', 'rb')\n",
        "  encdata = pkl.load(f)\n",
        "  f.close()\n",
        "else:\n",
        "  encdata = datapipeline()"
      ],
      "metadata": {
        "id": "XfQ0x2Syocu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === DATA SITE SEQ ===\n",
        "is_verbose_training = False\n",
        "idata = 2\n",
        "#\n",
        "data = encdata[idata]\n",
        "xtrain, xtest, ytrain, ytest = tlrn.dataSplitRF(data)\n",
        "xtrainres, ytrainres = xtrain, ytrain"
      ],
      "metadata": {
        "id": "MsRCZItD1-3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# ========================================================\n",
        "#                      SKLEARN MODELS\n",
        "# ========================================================\n",
        "# ========================================================\n",
        "is_mlp = True\n",
        "is_mlp2 = True\n",
        "is_rf = True\n",
        "is_lr = True\n",
        "if issklearnmodels:\n",
        "  if is_mlp:\n",
        "    # === MLP 1 LAYER ===\n",
        "    print('=== MLP 1 LAYER TRAINING ===')\n",
        "    clf = MLP(\n",
        "      hidden_layer_sizes=(94,),\n",
        "      activation = 'logistic', solver = 'adam',\n",
        "      learning_rate = 'invscaling',\n",
        "      early_stopping = False,\n",
        "      max_iter = 1000, random_state = 0 )\n",
        "    mlp = modelpipeline(\n",
        "        clf,\n",
        "        xtrainres, ytrainres,\n",
        "        xtest, ytest, is_verbose_training\n",
        "      ).modelTrain()\n",
        "    mlp = mlp.modelPredict()\n",
        "  #\n",
        "  if is_mlp2:\n",
        "    # === MLP 2 LAYERS ===\n",
        "    print('=== MLP 2 LAYERS TRAINING ===')\n",
        "    clf = MLP(\n",
        "      hidden_layer_sizes=( 250, 25 ),\n",
        "      activation = 'logistic', solver = 'adam',\n",
        "      learning_rate = 'invscaling',\n",
        "      early_stopping = False,\n",
        "      max_iter = 1000, random_state = 0\n",
        "    )\n",
        "    mlp_2layers = modelpipeline(\n",
        "        clf,\n",
        "        xtrainres, ytrainres,\n",
        "        xtest, ytest, is_verbose_training\n",
        "      ).modelTrain()\n",
        "    mlp_2layers = mlp_2layers.modelPredict()\n",
        "  #\n",
        "  if is_rf:\n",
        "    # === RANDOM FOREST ===\n",
        "    print('=== RANDOM FOREST TRAINING ===')\n",
        "    clf = RandomForestClassifier(\n",
        "      n_estimators = 784, criterion = 'entropy',\n",
        "      n_jobs = -1, random_state = 0)\n",
        "    rf = modelpipeline(\n",
        "        clf,\n",
        "        xtrainres, ytrainres,\n",
        "        xtest, ytest, is_verbose_training\n",
        "      ).modelTrain()\n",
        "    rf = rf.modelPredict()\n",
        "  #\n",
        "  if is_lr:\n",
        "    # === LOGISTIC REGRESSION ===\n",
        "    print('=== LOGISTIC REGRESSION TRAINING ===')\n",
        "    clf = LogisticRegression(\n",
        "      solver = 'sag', penalty = 'l2',\n",
        "      C = 0.233572, max_iter = 10000,\n",
        "      n_jobs = -1, random_state = 0)\n",
        "    lr = modelpipeline(\n",
        "        clf,\n",
        "        xtrainres, ytrainres,\n",
        "        xtest, ytest, is_verbose_training\n",
        "      ).modelTrain()\n",
        "    lr = lr.modelPredict()\n",
        "  #\n",
        "  if issavemodels:\n",
        "    print('=== SAVING SKLEARN MODELS ===')\n",
        "    if is_mlp:\n",
        "      dump(mlp, path_to_module+'/saved_models/mlp_siteseq.joblib')\n",
        "    if is_mlp2:\n",
        "      dump(mlp_2layers, path_to_module+'/saved_models/mlp_2layers_siteseq.joblib')\n",
        "    if is_rf:\n",
        "      dump(rf, path_to_module+'/saved_models/rf_siteseq.joblib')\n",
        "    if is_lr:\n",
        "      dump(lr, path_to_module+'/saved_models/lr_siteseq.joblib')"
      ],
      "metadata": {
        "id": "DWACnvXIK1sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# ========================================================\n",
        "#                      FNNS MODELS\n",
        "# ========================================================\n",
        "# ========================================================\n",
        "if isfnnmodels:\n",
        "  max_iter_training = 25\n",
        "  # === DATA FOR FFNS ===\n",
        "  data_test_size = tf_models.define_data_test_size(idata)\n",
        "  x_train_ffns, x_test_ffns, y_train_ffns, y_test_ffns = tf_models.data_split_for_fnns(\n",
        "    encdata[idata],\n",
        "    testsize = data_test_size\n",
        "  )\n",
        "  x_boot_ffns, y_boot_ffns = tf_models.bootstrapForHypertuning( encdata[idata] )\n",
        "  data_dict = {\n",
        "    'x_train': x_train_ffns, 'y_train': y_train_ffns,\n",
        "    'x_boot': x_boot_ffns, 'y_boot': y_boot_ffns,\n",
        "    'x_test': x_test_ffns, 'y_test': y_test_ffns\n",
        "  }\n",
        "  # === FFN 3 LAYERS ===\n",
        "  best_ffn3_roc = 0\n",
        "  for iter in range(max_iter_training):\n",
        "    params_grid_ffn3 = {\n",
        "      'unit_layer_1': 64,\n",
        "      'unit_layer_2': 100,\n",
        "      'unit_layer_3': 16,\n",
        "      'unit_dropout_1': 0,\n",
        "      'is_batch_normalization_1': True,\n",
        "      'unit_batch': 32\n",
        "    }\n",
        "    ffn3_tmp = tf_models.dlModelsPipeline(\n",
        "      tf_models.model_ffn3,\n",
        "      data_dict,\n",
        "      params_grid_ffn3\n",
        "    )\n",
        "    ffn3_tmp.modelTrain()\n",
        "    ffn3_tmp.modelPredict()\n",
        "    if ffn3_tmp.roc_auc > best_ffn3_roc:\n",
        "      best_ffn3_roc = ffn3_tmp.roc_auc\n",
        "      ffn3 = ffn3_tmp\n",
        "  print('> best roc auc FFN3:', ffn3.roc_auc)\n",
        "  #\n",
        "  # === FFN 5 LAYERS ===\n",
        "  best_ffn5_roc = 0\n",
        "  for iter in range(max_iter_training):\n",
        "    params_grid_ffn5 = {\n",
        "      'unit_layer_1': 100,\n",
        "      'unit_layer_2': 8,\n",
        "      'unit_layer_3': 128,\n",
        "      'unit_layer_4': 100,\n",
        "      'unit_layer_5': 32,\n",
        "      'unit_dropout_1': 0.1,\n",
        "      'unit_dropout_2': 0.05,\n",
        "      'is_batch_normalization_1': True,\n",
        "      'is_batch_normalization_2': True,\n",
        "      'unit_batch': 32\n",
        "    }\n",
        "    ffn5_tmp = tf_models.dlModelsPipeline(\n",
        "      tf_models.model_ffn5,\n",
        "      data_dict,\n",
        "      params_grid_ffn5\n",
        "    )\n",
        "    ffn5_tmp.modelTrain()\n",
        "    ffn5_tmp.modelPredict()\n",
        "    if ffn5_tmp.roc_auc > best_ffn5_roc:\n",
        "      best_ffn5_roc = ffn5_tmp.roc_auc\n",
        "      ffn5 = ffn5_tmp\n",
        "  print('> best roc auc FFN5:', ffn5.roc_auc)\n",
        "  #\n",
        "  # === FFN 10 LAYERS ===\n",
        "  best_ffn10_roc = 0\n",
        "  for iter in range(max_iter_training):\n",
        "    params_grid_ffn10 = {\n",
        "      'unit_layer_1': 64,\n",
        "      'unit_layer_2': 128,\n",
        "      'unit_layer_3': 32,\n",
        "      'unit_layer_4': 256,\n",
        "      'unit_layer_5': 32,\n",
        "      'unit_layer_6': 32,\n",
        "      'unit_layer_7': 128,\n",
        "      'unit_layer_8': 128,\n",
        "      'unit_layer_9': 64,\n",
        "      'unit_layer_10': 64,\n",
        "      'unit_dropout_1': 0.15,\n",
        "      'unit_dropout_2': 0.2,\n",
        "      'unit_dropout_3': 0.05,\n",
        "      'unit_dropout_4': 0.15,\n",
        "      'is_batch_normalization_1': True,\n",
        "      'is_batch_normalization_2': True,\n",
        "      'is_batch_normalization_3': True,\n",
        "      'is_batch_normalization_4': True,\n",
        "      'is_batch_normalization_5': True,\n",
        "      'is_batch_normalization_6': True,\n",
        "      'unit_batch': 64\n",
        "    }\n",
        "    ffn10_tmp = tf_models.dlModelsPipeline(\n",
        "      tf_models.model_ffn10,\n",
        "      data_dict,\n",
        "      params_grid_ffn10\n",
        "    )\n",
        "    ffn10_tmp.modelTrain()\n",
        "    ffn10_tmp.modelPredict()\n",
        "    if ffn10_tmp.roc_auc > best_ffn10_roc:\n",
        "      best_ffn10_roc = ffn10_tmp.roc_auc\n",
        "      ffn10 = ffn10_tmp\n",
        "  print('> best roc auc FFN10:', ffn10.roc_auc)\n",
        "  #\n",
        "  # === SAVING FFNS ===\n",
        "  if issavemodels:\n",
        "    print('=== SAVING FNNS MODELS ===')\n",
        "    ffn3.trained_model.save(path_to_module+'/saved_models/ffn3_siteseq')\n",
        "    ffn5.trained_model.save(path_to_module+'/saved_models/ffn5_siteseq')\n",
        "    ffn10.trained_model.save(path_to_module+'/saved_models/ffn10_siteseq')"
      ],
      "metadata": {
        "id": "TTar5QxnzFPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# ========================================================\n",
        "#                      CNNS MODELS\n",
        "# ========================================================\n",
        "# ========================================================\n",
        "if iscnnmodels:\n",
        "  max_iter_training = 25\n",
        "  # === DATA FOR CNNS ===\n",
        "  data_test_size = tf_models.define_data_test_size(idata)\n",
        "  x_train_cnns, x_test_cnns, y_train_cnns, y_test_cnns = tf_models.data_split_for_cnns(\n",
        "    encdata[idata],\n",
        "    testsize = data_test_size\n",
        "  )\n",
        "  x_train_cnns = x_train_cnns.reshape(-1, 24, 7, 1)\n",
        "  x_test_cnns = x_test_cnns.reshape(-1, 24, 7, 1)\n",
        "  x_boot_cnns, y_boot_cnns = tf_models.bootstrapForHypertuning( encdata[idata] )\n",
        "  x_boot_cnns = x_boot_cnns.reshape(-1, 24, 7, 1)\n",
        "  data_dict = {\n",
        "    'x_train': x_train_cnns, 'y_train': y_train_cnns,\n",
        "    'x_boot': x_boot_cnns, 'y_boot': y_boot_cnns,\n",
        "    'x_test': x_test_cnns, 'y_test': y_test_cnns\n",
        "  }\n",
        "  # === CNN 3 LAYERS ===\n",
        "  best_cnn3_roc = 0\n",
        "  for iter in range(max_iter_training):\n",
        "    params_grid_cnn3 = {\n",
        "      'unit_layer_1' : 256,\n",
        "      'unit_layer_2' : 128,\n",
        "      'activation_layer_1' : 'softmax',\n",
        "      'activation_layer_2' : 'tanh',\n",
        "      'activation_layer_3' : 'tanh',\n",
        "      'unit_dropout_1' : 0.1,\n",
        "      'unit_dropout_2' : 0.05,\n",
        "      'is_batch_normalization_1' : True,\n",
        "      'is_batch_normalization_2' : True,\n",
        "      'unit_batch' : 128}\n",
        "    cnn3_tmp = tf_models.dlModelsPipeline(\n",
        "      tf_models.model_cnn3,\n",
        "      data_dict,\n",
        "      params_grid_cnn3\n",
        "    )\n",
        "    cnn3_tmp.modelTrain()\n",
        "    cnn3_tmp.modelPredict()\n",
        "    if cnn3_tmp.roc_auc > best_cnn3_roc:\n",
        "      best_cnn3_roc = cnn3_tmp.roc_auc\n",
        "      cnn3 = cnn3_tmp\n",
        "  print('> best roc auc CNN3:', cnn3.roc_auc)\n",
        "  #\n",
        "  # === CNN 5 LAYERS ===\n",
        "  best_cnn5_roc = 0\n",
        "  for iter in range(max_iter_training):\n",
        "    params_grid_cnn5 = {\n",
        "      'unit_layer_1' : 128,\n",
        "      'unit_layer_2' : 256,\n",
        "      'unit_layer_3' : 75,\n",
        "      'unit_layer_4' : 64,\n",
        "      'activation_layer_1' : 'tanh',\n",
        "      'activation_layer_2' : 'softmax',\n",
        "      'activation_layer_3' : 'softmax',\n",
        "      'activation_layer_4' : 'tanh',\n",
        "      'activation_layer_5' : 'relu',\n",
        "      'unit_dropout_1' : 0.15,\n",
        "      'unit_dropout_2' : 0.1,\n",
        "      'is_batch_normalization_1' : True,\n",
        "      'is_batch_normalization_2' : True,\n",
        "      'is_batch_normalization_3' : True,\n",
        "      'unit_batch' : 128}\n",
        "    cnn5_tmp = tf_models.dlModelsPipeline(\n",
        "      tf_models.model_cnn5,\n",
        "      data_dict,\n",
        "      params_grid_cnn5\n",
        "    )\n",
        "    cnn5_tmp.modelTrain()\n",
        "    cnn5_tmp.modelPredict()\n",
        "    if cnn5_tmp.roc_auc > best_cnn5_roc:\n",
        "      best_cnn5_roc = cnn5_tmp.roc_auc\n",
        "      cnn5 = cnn5_tmp\n",
        "  print('> best roc auc CNN5:', cnn5.roc_auc)\n",
        "  #\n",
        "  # === CNN 10 LAYERS ===\n",
        "  best_cnn10_roc = 0\n",
        "  for iter in range(max_iter_training):\n",
        "    params_grid_cnn10 = {\n",
        "      'unit_layer_1' : 100,\n",
        "      'unit_layer_2' : 200,\n",
        "      'unit_layer_3' : 256,\n",
        "      'unit_layer_4' : 100,\n",
        "      'unit_layer_5' : 75,\n",
        "      'unit_layer_6' : 8,\n",
        "      'unit_layer_7' : 75,\n",
        "      'unit_layer_8' : 32,\n",
        "      'unit_layer_9' : 8,\n",
        "      'activation_layer_1' : 'tanh',\n",
        "      'activation_layer_2' : 'softmax',\n",
        "      'activation_layer_3' : 'relu',\n",
        "      'activation_layer_4' : 'softmax',\n",
        "      'activation_layer_5' : 'softmax',\n",
        "      'activation_layer_6' : 'tanh',\n",
        "      'activation_layer_7' : 'softmax',\n",
        "      'activation_layer_8' : 'relu',\n",
        "      'activation_layer_9' : 'softmax',\n",
        "      'activation_layer_10' : 'relu',\n",
        "      'unit_dropout_1' : 0.05,\n",
        "      'unit_dropout_2' : 0.1,\n",
        "      'unit_dropout_3' : 0.1,\n",
        "      'unit_dropout_4' : 0,\n",
        "      'unit_dropout_5' : 0.1,\n",
        "      'unit_dropout_6' : 0,\n",
        "      'is_batch_normalization_1' : True,\n",
        "      'is_batch_normalization_2' : True,\n",
        "      'is_batch_normalization_3' : True,\n",
        "      'is_batch_normalization_4' : True,\n",
        "      'is_batch_normalization_5' : True,\n",
        "      'is_batch_normalization_6' : True,\n",
        "      'is_batch_normalization_7' : True,\n",
        "      'unit_batch' : 256}\n",
        "    cnn10_tmp = tf_models.dlModelsPipeline(\n",
        "      tf_models.model_cnn10,\n",
        "      data_dict,\n",
        "      params_grid_cnn10\n",
        "    )\n",
        "    cnn10_tmp.modelTrain()\n",
        "    cnn10_tmp.modelPredict()\n",
        "    if cnn10_tmp.roc_auc > best_cnn10_roc:\n",
        "      best_cnn10_roc = cnn10_tmp.roc_auc\n",
        "      cnn10 = cnn10_tmp\n",
        "  print('> best roc auc CNN10:', cnn10.roc_auc)\n",
        "  #\n",
        "  # === SAVING CNNS ===\n",
        "  if issavemodels:\n",
        "    print('=== SAVING CNNS MODELS ===')\n",
        "    cnn3.trained_model.save(path_to_module+'/saved_models/cnn3_siteseq')\n",
        "    cnn5.trained_model.save(path_to_module+'/saved_models/cnn5_siteseq')\n",
        "    cnn10.trained_model.save(path_to_module+'/saved_models/cnn10_siteseq')"
      ],
      "metadata": {
        "id": "ySrkbpnuo0H_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fba5efc8-2f10-4cd7-a3f7-0ce9fcf8541d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN3 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> best roc auc CNN3: 0.79\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 1s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> CNN5 training\n",
            "171/171 [==============================] - 0s 2ms/step\n",
            "> best roc auc CNN5: 0.809\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 4ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> CNN10 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> best roc auc CNN10: 0.768\n",
            "=== SAVING CNNS MODELS ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# ========================================================\n",
        "#                      RNNS MODELS\n",
        "# ========================================================\n",
        "# ========================================================\n",
        "if isrnnmodels:\n",
        "  max_iter_training = 25\n",
        "  # === DATA FOR RNNS ===\n",
        "  data_test_size = tf_models.define_data_test_size(idata)\n",
        "  x_train_rnns, x_test_rnns, y_train_rnns, y_test_rnns = tf_models.data_split_for_cnns(\n",
        "    encdata[idata], testsize = data_test_size\n",
        "  )\n",
        "  x_boot_rnns, y_boot_rnns = tf_models.bootstrapForHypertuning(\n",
        "    encdata[idata], is_reshape = False\n",
        "  )\n",
        "  data_dict = {\n",
        "    'x_train': x_train_rnns, 'y_train': y_train_rnns,\n",
        "    'x_boot': x_boot_rnns, 'y_boot': y_boot_rnns,\n",
        "    'x_test': x_test_rnns, 'y_test': y_test_rnns\n",
        "  }\n",
        "  # === LSTM 3 LAYERS ===\n",
        "  best_lstm3_roc = 0\n",
        "  for iter in range(max_iter_training):\n",
        "    params_grid_lstm3 = {\n",
        "      'unit_layer_1': 200,\n",
        "      'unit_layer_2': 100,\n",
        "      'activation_layer_1': 'tanh',\n",
        "      'activation_layer_2': 'relu',\n",
        "      'activation_layer_3': 'relu',\n",
        "      'unit_dropout_1': 0.05,\n",
        "      'unit_dropout_2': 0.05,\n",
        "      'is_batch_normalization_1': True,\n",
        "      'is_batch_normalization_2': True,\n",
        "      'unit_batch': 32\n",
        "    }\n",
        "    lstm3_tmp = tf_models.dlModelsPipeline(\n",
        "      tf_models.model_lstm3,\n",
        "      data_dict,\n",
        "      params_grid_lstm3\n",
        "    )\n",
        "    lstm3_tmp.modelTrain()\n",
        "    lstm3_tmp.modelPredict()\n",
        "    if lstm3_tmp.roc_auc > best_lstm3_roc:\n",
        "      best_lstm3_roc = lstm3_tmp.roc_auc\n",
        "      lstm3 = lstm3_tmp\n",
        "  print('> best roc auc LSTM3:', lstm3.roc_auc)\n",
        "  # === GRU 3 LAYERS ===\n",
        "  best_gru3_roc = 0\n",
        "  for iter in range(max_iter_training):\n",
        "    params_grid_gru3 = {\n",
        "      'unit_layer_1': 256,\n",
        "      'unit_layer_2': 128,\n",
        "      'activation_layer_1': 'tanh',\n",
        "      'activation_layer_2': 'relu',\n",
        "      'activation_layer_3': 'tanh',\n",
        "      'unit_dropout_1': 0.05,\n",
        "      'unit_dropout_2': 0.15,\n",
        "      'is_batch_normalization_1': True,\n",
        "      'is_batch_normalization_2': True,\n",
        "      'unit_batch': 32\n",
        "    }\n",
        "    gru3_tmp = tf_models.dlModelsPipeline(\n",
        "      tf_models.model_gru3,\n",
        "      data_dict,\n",
        "      params_grid_gru3\n",
        "    )\n",
        "    gru3_tmp.modelTrain()\n",
        "    gru3_tmp.modelPredict()\n",
        "    if gru3_tmp.roc_auc > best_gru3_roc:\n",
        "      best_gru3_roc = gru3_tmp.roc_auc\n",
        "      gru3 = gru3_tmp\n",
        "  print('> best roc auc GRU3:', gru3.roc_auc)\n",
        "  #\n",
        "\n",
        "  # === SAVING RNNS ===\n",
        "  if issavemodels:\n",
        "    print('=== SAVING RNNS MODELS ===')\n",
        "    lstm3.trained_model.save(path_to_module+'/saved_models/lstm3_siteseq')\n",
        "    gru3.trained_model.save(path_to_module+'/saved_models/gru3_siteseq')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxBiumvLWCzD",
        "outputId": "c94bff32-2ba1-423c-9d4c-f10a2c9de8d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 4ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 4ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> LSTM3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> best roc auc LSTM3: 0.862\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 4ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> GRU3 training\n",
            "171/171 [==============================] - 1s 3ms/step\n",
            "> best roc auc GRU3: 0.852\n",
            "=== SAVING RNNS MODELS ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_18_layer_call_fn, lstm_cell_18_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}